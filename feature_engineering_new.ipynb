{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel Data Analysis\n",
    "\n",
    "In this demo, we will be doing some demos on temporal feature engineering with the Kaggle Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all of the files you are given\n",
    "df_tr = pd.read_csv(\"train.csv\")\n",
    "df_public_test = pd.read_csv(\"test_public.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710670\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "print(len(df_tr))\n",
    "print(len(df_public_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(df_tr.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_holiday(timestamp):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set has {df_tr['MISSING_DATA'].sum()} incomplete datapoints\")\n",
    "print(f\"Public test set has {df_public_test['MISSING_DATA'].sum()} incomplete datapoints\")\n",
    "\n",
    "print(\"train CALL_TYPE == A:\", (df_tr[\"CALL_TYPE\"] == \"A\").sum() / len(df_tr))\n",
    "print(\"train CALL_TYPE == B:\", (df_tr[\"CALL_TYPE\"] == \"B\").sum() / len(df_tr))\n",
    "print(\"train CALL_TYPE == C:\", (df_tr[\"CALL_TYPE\"] == \"C\").sum() / len(df_tr))\n",
    "\n",
    "print(\"public CALL_TYPE == A:\", (df_public_test[\"CALL_TYPE\"] == \"A\").sum() / len(df_public_test))\n",
    "print(\"public CALL_TYPE == B:\", (df_public_test[\"CALL_TYPE\"] == \"B\").sum() / len(df_public_test))\n",
    "print(\"public CALL_TYPE == C:\", (df_public_test[\"CALL_TYPE\"] == \"C\").sum() / len(df_public_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"train set:\")\n",
    "for col in [\"TRIP_ID\",\"CALL_TYPE\",\"ORIGIN_CALL\",\"ORIGIN_STAND\",\"TAXI_ID\",\"TIMESTAMP\",\"DAY_TYPE\",\"MISSING_DATA\"]:\n",
    "    print(f'{col} has {df_tr[col].isnull().sum()} null rows')\n",
    "\n",
    "print(\"\\npublic test set:\")    \n",
    "for col in [\"TRIP_ID\",\"CALL_TYPE\",\"ORIGIN_CALL\",\"ORIGIN_STAND\",\"TAXI_ID\",\"TIMESTAMP\",\"DAY_TYPE\",\"MISSING_DATA\"]:\n",
    "    print(f'{col} has {df_public_test[col].isnull().sum()} null rows')\n",
    "print()\n",
    "temp = df_tr[df_tr[\"CALL_TYPE\"] == \"A\"]\n",
    "print(f\"CALL_TYPE A has {temp['ORIGIN_CALL'].isnull().sum()} missing ORIGIN_CALLs\")\n",
    "temp = df_tr[df_tr[\"CALL_TYPE\"] == \"B\"]\n",
    "print(f\"CALL_TYPE B has {temp['ORIGIN_STAND'].isnull().sum()} missing ORIGIN_STANDs\")\n",
    "temp = df_public_test[df_public_test[\"CALL_TYPE\"] == \"A\"]\n",
    "print(f\"In public test, CALL_TYPE A has {temp['ORIGIN_CALL'].isnull().sum()} missing ORIGIN_CALLs\")\n",
    "temp = df_public_test[df_public_test[\"CALL_TYPE\"] == \"B\"]\n",
    "print(f\"In public test, CALL_TYPE B has {temp['ORIGIN_STAND'].isnull().sum()} missing ORIGIN_STANDs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing \n",
    "data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n",
    "\n",
    "We are not doing anything with the MISSING_DATA. It is up to you to find a way to use (or ignore) that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamp(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x)\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "def is_holiday(timestamp):\n",
    "    yr, mon, day, _hr, _wk = parse_timestamp(timestamp)\n",
    "#     print(f\"year {yr}, month {mon}, day {day}, hour {_hr}, week {_wk}\")\n",
    "    holidays = [\n",
    "        (2013, 1, 1),\n",
    "        (2014, 1, 1),\n",
    "        (2013, 2, 12),\n",
    "        (2014, 3, 4),\n",
    "        (2013, 3, 29),\n",
    "        (2014, 4, 18),\n",
    "        (2013, 3, 31),\n",
    "        (2014, 4, 20),\n",
    "        (2013, 4, 25),\n",
    "        (2014, 4, 25),\n",
    "        (2013, 5, 1),\n",
    "        (2014, 5, 1),\n",
    "        (2013, 6, 10),\n",
    "        (2014, 6, 10),\n",
    "        (2013, 8, 15),\n",
    "        (2014, 8, 15),\n",
    "        (2013, 10, 5),\n",
    "        (2014, 10, 5),\n",
    "        (2013, 11, 1),\n",
    "        (2014, 11, 1),\n",
    "        (2013, 12, 1),\n",
    "        (2014, 12, 1),\n",
    "        (2013, 12, 25),\n",
    "        (2014, 12, 25)\n",
    "    ]\n",
    "    if (yr, mon, day) in holidays:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_holidays_eve(timestamp):\n",
    "    tomorrow = timestamp + (60 * 60 * 24)\n",
    "    return is_holiday(tomorrow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parse_time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do some Feature Analysis\n",
    "\n",
    "For our feature analysis, we are looking at which of our engineered features may be useful in making a taxicab time regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Prediction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = df_tr[\"LEN\"].mean(), df_tr[\"LEN\"].std()\n",
    "median = df_tr[\"LEN\"].median()\n",
    "print(f\"{mean=} {median=} {std=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample submission file that is given on kaggle\n",
    "df_sample = pd.read_csv(\"sampleSubmission.csv\")\n",
    "\n",
    "df_sample[\"TRAVEL_TIME\"] = 716.43\n",
    "\n",
    "# mean(716.43) -> 792.73593\n",
    "# median(600) -> 784.74219\n",
    "# df_sample.to_csv(\"my_pred.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First n samples to analyze. Set to -1 to use all data\n",
    "end = -1\n",
    "\n",
    "outlier_threshold = 3\n",
    "\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed = df_tr[df_tr[\"LEN\"] < mean + outlier_threshold * std]\n",
    "\n",
    "# Because our y-values only take on multiples of 15, we want just enough buckets in a histogram\n",
    "# such that each buckets counts one value's frequency. (e.x. one bucket counts how many 15s trips, \n",
    "# how many 30s trips, etc. )\n",
    "buckets = (int(mean + outlier_threshold * std) // 15)\n",
    "\n",
    "print(f\"Using: {len(df_trimmed)}/{len(df_tr)}\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(18,14))\n",
    "\n",
    "# Now, we visualize some features that we think might be useful\n",
    "for idx, v in enumerate([\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"ORIGIN_STAND\", \"ORIGIN_CALL\",\n",
    "                        \"TAXI_ID\"]):\n",
    "  # idx // 3 = row, idx % 3 = column\n",
    "  ax = axs[idx // 3, idx % 3]\n",
    "  \n",
    "  # Remove any rows with invalid values\n",
    "  df_subset = df_trimmed.dropna(subset=v)\n",
    "  \n",
    "  # Create a histogram. Look up the documentation for more details\n",
    "  ax.hist2d(df_subset[v][:end], df_subset[\"LEN\"][:end], cmap=\"CMRmap\", bins=(120,buckets))\n",
    "  \n",
    "  # Some stylistic things to make the graphs look nice\n",
    "  ax.set_xlim(ax.get_xlim()[0] - 1, ax.get_xlim()[1] + 1)\n",
    "  ax.set_facecolor(\"black\")\n",
    "  ax.set_ylabel(\"seconds\", fontsize=18)\n",
    "  ax.set_title(f\"Feature: {v}\", fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for v in [0, 5, 11, 17, 23]:\n",
    "  # Filter data where the HR matches v\n",
    "  hourly_data = df_trimmed[df_trimmed[\"HR\"] == v][\"LEN\"]\n",
    "  histogram, bin_boundary = np.histogram(hourly_data, bins=buckets)\n",
    "  histogram = histogram / len(hourly_data)\n",
    "  # The center is the left_bound and right_bound of a bucket\n",
    "  bin_centers = [(bin_boundary[i] + bin_boundary[i + 1]) / 2 for i in range(buckets)]\n",
    "  plt.plot(bin_centers, histogram, label=f\"HR={v}\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bar_chart(column=\"MON\"):\n",
    "#     plt.figure(figsize=(4,4))\n",
    "#     months = sorted(df_tr[column].unique())\n",
    "#     month_data = []\n",
    "#     for v in months:\n",
    "#         month_data.append(df_trimmed[df_trimmed[column] == v][\"LEN\"].mean())\n",
    "#     plt.bar(months, month_data)\n",
    "#     plt.xlabel(column)\n",
    "#     plt.ylabel(\"LEN\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(18,14))\n",
    "\n",
    "\n",
    "for idx, column in enumerate([\"YR\", \"MON\", \"DAY\", \"WK\"]):\n",
    "    # idx // 3 = row, idx % 3 = column\n",
    "    print(idx)\n",
    "    ax = axs[idx // 2, idx % 2]\n",
    "    \n",
    "#     ax.figure(figsize=(4,4))\n",
    "    months = sorted(df_tr[column].unique())\n",
    "    month_data = []\n",
    "    for v in months:\n",
    "        month_data.append(df_trimmed[df_trimmed[column] == v][\"LEN\"].mean())\n",
    "    ax.bar(months, month_data)\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel(\"LEN\")\n",
    "    ax.set_title(column)\n",
    "    \n",
    "# ax.set_xlim(ax.get_xlim()[0] - 1, ax.get_xlim()[1] + 1)\n",
    "#   ax.set_facecolor(\"black\")\n",
    "#   ax.set_ylabel(\"seconds\", fontsize=18)\n",
    "#   ax.set_title(f\"Feature: {v}\", fontsize=20)\n",
    "  \n",
    "  # Remove any rows with invalid values\n",
    "#   df_subset = df_trimmed.dropna(subset=v)\n",
    "# bar_chart(\"MON\")\n",
    "# bar_chart(\"DAY\")\n",
    "# bar_chart(\"WK\")\n",
    "# bar_chart(\"YR\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_start_date = pd.to_datetime(df_tr[\"TIMESTAMP\"], unit='s').min()\n",
    "tr_end_date = pd.to_datetime(df_tr[\"TIMESTAMP\"], unit='s').max()\n",
    "tt_start_date = pd.to_datetime(df_public_test[\"TIMESTAMP\"], unit='s').min()\n",
    "tt_end_date = pd.to_datetime(df_public_test[\"TIMESTAMP\"], unit='s').max()\n",
    "print(f\"The training data have dates ranging from {tr_start_date} to {tr_end_date}.\")\n",
    "print(f\"The public test data have dates ranging from {tt_start_date} to {tt_end_date}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_trimmed['LEN'], bins=200)\n",
    "plt.xlabel('LEN')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('training data distribution of LEN (outliers removed)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the actual coordinates, not just LEN\n",
    "def parse_polyline(polyline):\n",
    "    assert not \" \" in polyline, \"polyline has whitespace!\"\n",
    "    polyline = polyline.strip(\"[]\")\n",
    "    coords = polyline.split(\"],[\")\n",
    "    for i in range(len(coords)):\n",
    "        coords[i] = coords[i].split(\",\")\n",
    "        if len(coords[i]) < 2:\n",
    "            return []\n",
    "        coords[i][0] = float(coords[i][0])\n",
    "        coords[i][1] = float(coords[i][1])\n",
    "    return coords\n",
    "\n",
    "def get_start(coords):\n",
    "    if len(coords) == 0:\n",
    "        return None, None\n",
    "    return coords[0][0], coords[0][1]\n",
    "\n",
    "def get_end(coords):\n",
    "    if len(coords) == 0:\n",
    "        return None, None\n",
    "    return coords[-1][0], coords[-1][1]\n",
    "\n",
    "def get_start_lat(polyline):\n",
    "    parsed = parse_polyline(polyline)\n",
    "    return get_start(parsed)[0]\n",
    "\n",
    "def get_start_lon(polyline):\n",
    "    parsed = parse_polyline(polyline)\n",
    "    return get_start(parsed)[1]    \n",
    "\n",
    "x_coords = []\n",
    "y_coords = []\n",
    "for index, row in df_trimmed.iterrows():\n",
    "    if index > 1000:\n",
    "        break\n",
    "    parsed = parse_polyline(row[\"POLYLINE\"])\n",
    "    for coord in parsed:\n",
    "        if coord == None or len(coord) != 2 or coord[0] == None or coord[1] == None:\n",
    "            continue\n",
    "        x_coords.append(coord[0])\n",
    "        y_coords.append(coord[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_tr[\"START_LAT\"] = df_tr[\"POLYLINE\"].apply(get_start_lat)\n",
    "# df_tr[\"START_LON\"] = df_tr[\"POLYLINE\"].apply(get_start_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 30\n",
    "\n",
    "min_y, max_y = min(y_coords), max(y_coords)\n",
    "min_x, max_x = min(x_coords), max(x_coords)\n",
    "# min_y, max_y = min(min(y_coords), df_tr[\"START_LAT\"].min()), max(max(y_coords), df_tr[\"START_LAT\"].max())\n",
    "# min_x, max_x = min(min(x_coords), df_tr[\"START_LON\"].min()), max(max(x_coords), df_tr[\"START_LON\"].max())\n",
    "\n",
    "\n",
    "inc_y = (max_y - min_y) / grid_size\n",
    "inc_x = (max_x - min_x) / grid_size\n",
    "\n",
    "\n",
    "grid = np.zeros((grid_size, grid_size))\n",
    "\n",
    "for x, y in zip(x_coords, y_coords):\n",
    "    x_index = int((x - min_x) // (inc_x + 0.0001))\n",
    "    y_index = int((y - min_y) // (inc_y + 0.0001))\n",
    "    \n",
    "    if x_index >= grid_size or y_index >= grid_size:\n",
    "#         print(x_index, y_index)\n",
    "        continue\n",
    "\n",
    "    grid[x_index][y_index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because i can't figure out a cleaner way to set the labels...\n",
    "def remove_duplicates(labels):\n",
    "    seen_labels = []\n",
    "    for i, label in enumerate(labels):\n",
    "        if label in seen_labels:\n",
    "            labels[i] = \"\"\n",
    "        else:\n",
    "            seen_labels.append(label)\n",
    "    return labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "## Heatmap ###################\n",
    "##############################\n",
    "\n",
    "x_labels = remove_duplicates([\"{:.1f}\".format((inc_x * i) + min_x) for i in range(0, grid_size)])\n",
    "y_labels = remove_duplicates([\"{:.1f}\".format((inc_y * i) + min_y) for i in range(0, grid_size)])\n",
    "\n",
    "\n",
    "sns.heatmap(grid, cbar=False, xticklabels=x_labels, yticklabels=y_labels)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('Heatmap of Trip Positions')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
